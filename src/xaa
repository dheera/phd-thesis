\chapter{Single-photon imaging}

In our ghost imaging work described in the previous chapter, we found that averaging measurements over long dwell times is not the most efficient approach to imaging objects which have spatial structure. In particular by taking a small number of measurements and treating the results as an underdetermined linear system, and using computational optimization techniques to iteratatively find the most sparse solution in a suitable basis, it is possible to produce a high-quality image much faster than traditional averaging.

Ghost imaging, single pixel cameras, and other such non-traditional classical imagers are conceptually interesting but have limited application scope. In this chapter we pose the question of whether mainstream imaging devices can also benefit from similar computational reconstruction techniques. In particular capturing three-dimensional structure and reflectivity using a pulsed active illumination has many applications \cite{nicolas-applications,becker-fluorescence,mccarthy-kilometer}.

In this chapter, we look in particular at 3D light detection and ranging (LIDAR) systems \cite{mccarthy-kilometer,schwarz-mapping} and demonstrate an imager that uses similar sparsity-based computational imaging techniques to obtain high-quality depth and intensity maps using only a single photon per pixel. We also examine related experiments in imaging using single-photon detectors.

\section{First-photon imaging}

In LIDAR imaging, the scene is illuminated with a pulsed laser beam and the back-reflected light is measured by a time-resolving detector. LIDAR devices typically use Geiger-mode avalanche photodiodes (APDs) which resolve individual photon arrivals to within tens of picoseconds \cite{savage-single}, and obtain transverse spatial resolution either by using a single detector and raster-scanning the illumination one pixel at a time \cite{buller-ranging} or by flood-illuminating the scene and using a detector array \cite{jack-hgcdte}. However, in either case, it is typically necessary to collect hundreds of photon arrivals per pixel in order to perform accurate reflectivity imaging \cite{snyder-random,obrien-simulation} and tens of photon arrivals to obtain accurate depth maps \cite{pelligrini-laser,mccarthy-kilometer} even at negligible background light levels. In the presence of background noise, even more data needs to be collected in order to produce high-quality range images.

In this experiment we pose the question of what would happen if we only took a single photon arrival worth of data at each pixel of a LIDAR imager before immediately moving to the next pixel. In the ideal scenario of zero background noise, we would normally expect to have a range resolution equal to the pulse width, and a featureless intensity image since we only measure one arrival at each pixel. In the presence of background noise, the range information would be severely corrupted by any background photons.

However, traditional imaging by histogramming a large number of arrivals at each pixel does not take into account the spatial correlations found in real-world scenes, which both in depth and intensity typically feature smoothly-varying surfaces punctuated by few sharp edges compared to the number of pixels in the scene. Thus, similar to our work in compressed ghost imaging, we expect that drastically reducing our solution space combined with knowledge of the physics of low-flux measurements would make it possible to computationally recover a clean image from a relatively small number of measurements.

\begin{figure}[htb]
\centerline{\includegraphics[width=14cm]{figure-first-setup.pdf}}
\caption{Schematic of the raster-scanning imager used for first-photon imaging.}
\label{figure:first-setup}
\end{figure}

\subsection{Experimental setup and calibration}

In the early stages of the experiment, we performed raster scanning using a Discovery 4100 digital micromirror device (DMD) kit and a diode laser with a 1-ns pulse width. However, we found that raster scanning using a DMD resulted in too much loss of power (flood-illuminating a scene and selecting only 1 pixel on a megapixel array would imply a power loss on the order of $\sim 10^6$, putting our signal counts lower than the dark count rates of most single-photon APDs). Thus, with DMD-based raster scanning we were limited to operation only at very low resolutions. In addition, high-power laser diodes operating on sub-nanosecond time scales were not widely available. Although a mode-locked laser or optical amplification would be a viable solution to high power with short pulses, this is a comparatively much more expensive solution.

In addition, the DMD application programming interface was poorly documented and not easy to synchronize with photon-counting hardware at time scales shorter than $\sim$1 second per realization which is too slow for our purposes. Although the DMD can in principle update patterns much faster, upto rates of over 30 kHz, this mode of operation required preloading a full set of patterns onto the device with a limit of 2048 patterns before a software crash would occur, which is insufficient to raster-scan an image at square resolutions higher than 45$\times$45.

We opted instead to not use a DMD-based setup but instead perform raster scanning using a low power, collimated short pulse diode laser and a two-axis scanning galvo which yielded much higher power at each pixel, faster acquistion times, and simpler experimental design. Since the galvo mirrors accepted analog voltage inputs, it was easy to design a setup that synchronized them with photon-timing hardware, and with power independent of resolution, allowing us to easily scale our acquisition to megapixel levels of resolution.

A diagram of our raster-scanning active imager is shown in figure \ref{figure:first-setup}. The illumination source is a PicoQuant LDH series pulsed laser diode with a center wavelength of 640 nm, a tunable repetition rate of 5 - 80 MHz and set at 10 MHz for this experiment. As with most pulsed laser diode sources, the pulse shape and duration is dependent on the power, so we fix our power setting at 0.6 mW average, which gives us pulses with 226 ps RMS duration.

The laser output is reflected off a Thorlabs GVS012 two-axis scanning galvo system which is used to raster scan the beam over the target. The maximum mechanical scan angle is $\pm 20^\circ$ which is the limit on our field of view. The galvo system takes two analog voltage inputs (one for each axis, 0.5 V/deg) which we supply using a National Instruments NI USB-6008 module which contains two 12-Bit analog outputs with a maximum sample rate of 1 kHz.

We placed our objects at a distance of 1.5 to 2 m from the illumination source. The laser spot size at 2 m distance was measured to be 1.5 mm. Prior to detection, the light was filtered using a Andover custom free-space interference filter with 2 nm bandwidth centered at 640 nm and whose peak transmission was 49\%. The Geiger-mode APD was a Micro Photon Devices PDM series detector with 100 $\mu$m $\times$ 100 $\mu$m active area, 35\% quantum efficiency, less than 50 ps timing jitter, and less than $2\cdot 10^4$ dark counts per second. The photon detection events were time stamped relative to the laser pulse with 8 ps resolution using a PicoQuant HydraHarp TCSPC module. The objects to be imaged were placed at 1.5 m to 2.5 m distance from the optical setup. The laser and the single-photon detector were placed in the same horizontal plane, at a separation of 7 cm, making our imaging setup effectively monostatic.

Prior to measurement, a HydraHarp time-tagged time-resolved (T3) measurement was started over a network connection. Raster scanning was then performed by a custom Visual C\# program, which both corrects for perspective distortion from the mounting of the galvo mirror as well as alternates raster scanning direction as shown in figure \ref{figure:first-scanning} in order to in order to avoid abrupt mechanical transitions. Before each movement of the galvo mirror position, a pixel marker pulse was simultaneously output to a designated digital output of the USB-6008 which was connected to a Stanford Research Systems delay generator. The delay generator then generated a pulse compatible with the HydraHarp marker input in order to insert a marker into the data stream indicating a pixel position change. A final marker signal was output after the last pixel as well, accounting for a total of $N^2+1$ markers in each file for an $N \times N$-pixel image. The resulting HT3 files are read using a custom GNU C program (see Appendix \ref{appendix:ht3read}) based on PicoQuant file specifications, which discards the data prior to the first pixel and after the end of the measurement, and outputs a MATLAB-friendly ASCII file describing all the marker and photon events. For photon detection events, the C program additionally outputs the laser pulse number and the time bin at which the detection was received. For marker events, the C program outputs the total number of photons detected since the last marker and the sum of the time bin values since the last marker, allowing for convenient retrieval of the traditional averages (ground truth) measurement. The C program also has an option to only output the first few photon detections after each marker, since converting the entire .ht3 file to ASCII format would occupy several tens of gigabytes. The output ASCII format is illustrated in \ref{figure:first-outformat}.

\begin{figure}[htb]
\centerline{\includegraphics[width=9cm]{figure-first-scanning.pdf}}
\caption{Raster scanning was performed in alternating directions for each row in order to avoid abrupt transitions in translating the galvo mirrors.}
\label{figure:first-scanning}
\end{figure}

\begin{figure}[htb]
\centerline{\includegraphics[width=10cm]{figure-first-outformat.pdf}}
\caption{Description of the MATLAB-friendly .out format generated by our custom GNU C .ht3 file reader.}
\label{figure:first-outformat}
\end{figure}

\subsubsection{Focusing of laser beam}
As with most diode lasers, the output of the PicoQuant LDH series laser diode is multimodal and not very well collimated beyond a range of several tens of centimeters. Since a large spot size would reduce our transverse resolution, we mitigate this problem by using a pair of 12-cm focal length lenses to loosely focus the beam such that the spot size is small ($\sim$1 mm) over the 1.5-m to 2-m range where we place the object.

\subsubsection{Addition of background noise}
In order to simulate a realistic imaging scenario with background noise, we placed an incandescent lamp in the room pointed at the detector, and set the background noise level equal to the signal level (i.e. the probability of any detection being due to background noise would be around 50\%). We set this level by first turning off the incandescent source and using the laser to illuminate a reference point on a highly-reflective Lambertian surface at a distance of 2 m, measuring the average detected photon rate, and then matching that rate by tuning the current input to the incandescent source.

\subsubsection{Photon flux waveform measurement}
For range estimation, our computational imager requires knowledge of the laser pulseâ€™s
photon-flux waveform. We use $s(t)$ to denote the normalized ($\int s(t) dt = 1$) version of this
waveform for a laser pulse emitted at $t = 0$. This pulse shape was measured by directly
illuminating the detector with highly attenuated laser pulses and binning the photon arrival times
to generate a histogram of photon counts. Fitting a skewed Gamma function to this histogram
yielded:
\begin{equation}
s(t) \propto A (t - T_s)^4 exp\left( -\frac{t-T_s}{T_c} \right)
\end{equation}
where $T_s$ = 80 ps and $T_c$ = 40 ps, as shown in figure \ref{figure:first-pulse}.

\begin{figure}[htb]
\centerline{\includegraphics[width=14cm]{figure-first-pulse.pdf}}
\caption{Pulse shape of the PicoQuant LDH series laser diode obtained by histogramming, and skewed Gamma function fit.}
\label{figure:first-pulse}
\end{figure}

\subsubsection{Acquisition procedure}
To generate one complete data set, we raster scan over 1000 $\times$ 1000 pixels with the two-
axis galvo. At transverse pixel location $(x, y)$, we propose a first-photon imager that records only two values: $n(x, y)$, the number of laser pulses transmitted prior to the first detection event; and $t(x, y)$, the timing of the detection relative to the pulse that immediately preceded it. An ideal first-photon imager would move to the next pixel immediately after a single detection event.
In our case, it is not possible to build such a fast system since we are physically limited by the mechanical speed of our galvo mirror system; at the fastest scanning rate we typically observe tens or even hundreds of photon arrivals at each pixel. Faster scanning may be achievable using smaller galvo mirrors or digital micromirror arrays. We are also limited by the 1 kHz sampling rate of our digital-to-analog converter, which permits a fastest acquisition of a little under 17 minutes for a 1-megapixel image. Without these physical raster scanning limitations, our photon flux is high enough for much faster acquisition.

In order to compare our first-photon imaging method with traditional acquisition techniques, we need to perform a baseline measurement with traditional histogramming. In order to do this we deliberately slow down the acquisition to approximately 3 hours per image in order to collect enough photon arrivals at each pixel to obtain a clean histogram of a few thousand arrivals at each pixel. Our reference images are constructed from averaging over $\sim$1000 arrivals at each pixel, and are generated only for comparison purposes. For our novel first photon imaging method, we only extract the first arrival at each pixel and discard all subsequent data.

Because our entire scene was contained within a few meters of the imaging setup, our 100 ns pulse repetition period guaranteed that each non-background photon detection came from the immediately preceding laser pulse.

\subsection{Computational reconstruction procedure}

In this section we present a 3-step computational reconstruction procedure that we use to recover high quality scene reflectivity and depth data from the first photon arrival data. This reconstruction approach was led by Ahmed Kirmani, Andrea Colaco, Dongeek Shin, and Vivek Goyal from the Signal Transformation and Representation Group at MIT, who iteratively improved the approach with feedback from early experimental data.

\subsubsection{Reflectivity reconstruction}

Traditional imaging techniques typically count photon arrivals to measure reflectivity. However, given only a single arrival, the only information we obtain about reflectivity is the time duration (i.e. number of laser pulse repetition periods) until a photon is detected. Since brighter regions reflect a greater number of photons, the mean time duration until a photon is detected is shorter. This is characterized by Poisson statistics. Let $S$ be the average photon number in the back-reflected signal from a hypothetical unity-reflectivity spatial location illuminated by the laser pulse, $\alpha(x,y)$ denote the scene reflectivity at pixel location $(x,y)$, $B$ denote the background photon arrival rate, $T_r$ be the pulse repetition period, and $\gamma$ be the efficiency of the detector. The probability of not detecting a photon is then
\begin{equation}
P_0(x, y) = e^{-\gamma[\alpha(x,y)S + BT_r]}
\end{equation}
Since these probabilities are independent and identically distributed (iid) for each pulse, it follows that the number of pulses until a detection, denoted by $n(x, y)$, is geometrically distributed:
\begin{equation}
\operatorname{Pr}\left[n(x,y) = k \right] = P_0(x,y)^{k-1} \left[ 1 - P_0(x,y) \right]
\end{equation}

When there is no background light present, it follows that the maximum likelihood (ML) estimate of the reflectivity at each pixel $\hat{\alpha}_{ML}(x,y)$ is proportional to $1/n(x,y)$ for $n(x,y) \gg 1$. In the presence of high background light ($B T_r > \alpha(x,y) S$) this estimate becomes severely corrupted since it is much more likely that a background photon will reach the detector before a signal photon.

In order to perform a sparsity-promoting computational reconstruction of the reflectivity from this data, we first compute the log-likelihood of the ML estimate above, which is derived in \cite{kirmani-first}:
\begin{equation}
L\left( \alpha(x,y) | n(x,y) \right) = \gamma \left[ \alpha(x,y) S + B T_r \right] \left[ n(x,y) - 1 \right] - \log \left[ \gamma \alpha(x,y) S + B T_r \right]
\end{equation}

We must also choose a sparsity-promoting basis for reconstruction. Most objects are characterized by smoothly-varying surfaces punctuated by sharp edge transitions; this type of sparsity is captured well by most wavelet bases which are localized in both space and frequency. Bases such as the Fourier Transform or Discrete Cosine Transform which only localize frequency and not space require a large number of nonzero basis elements to describe sharp edges and are not typically the best choice for these types of objects, but may be a more suitable choice for imaging scenarios that do not contain edges. In our case we use the discrete wavelet transform (DWT) derived from Debauchie's 4-tap filters \cite{debauchies}. Representing the wavelet transform as $\Phi\left(\cdot\right)$ which is implemented in the form of matrix multiplication, we denote the wavelet coefficients as $\left\{ w(x,y) \right\} = \Phi(\left\{ \alpha(x,y) \right\})$ for tthe collection of reflectivity estimates $\left\{ \alpha(x,y) \right\}$ for all pixel coordinates $(x,y)$. A standard measure of sparsity, similar to that which we used in compressed sensing in the previous chapter, is the $\ell_1$-norm of these coefficients, defined by:
\begin{equation}
|| \Phi\left(\left\{ \alpha(x,y) \right\}\right) ||_1 = \sum_x \sum_y | w(x,y) |
\end{equation}

We would like to reconstruct the reflectivity estimate by minimizing a weighted sum of both the negative log-likelihood and the sparsity measuring function over the set of all possible images $\left\{(\alpha(x,y)\right\}$. This enforces that the estimate cannot deviate too far (quantified by the probability distribution itself) from the measured data, and that the estimate needs to be as spatially sparse as possible (quantified by the chosen DWT basis). The optimization program can be written as the minimization of
\begin{equation}
(1 - \beta) \left[ \sum_x \sum_y L( \alpha(x,y) | n(x,y) ) \right] + \beta || \Phi\left(\left\{ \alpha(x,y) \right\}\right) ||_1
\end{equation}
additionally enforcing that $\alpha(x,y) \geq 0$ for all $(x,y)$. Since the both the log-likelihood function is convex in $(x,y)$, and that the sparsity-promoting function is also convex \cite{blah}, their nonnegative weighted sum also becomes strictly convex. This enables us to efficiently find the global minimum solution to $\left\{ \alpha(x,y) \right\}$ by iteratively searching for a local minimum.

\subsubsection{Background noise censoring}

Before reconstructing the depth map of our object, we wish to eliminate the photon arrivals that are due to the background light and provide no information about depth. We do this by taking advantage of the fact that anomalous detections due to background light are independently and uniformly distributed over $[0, T_r]$ with a high variance of $T_r^2/12$ (following from the uniform probability distribution), whereas detections from back-reflected signal pulses are temporally concentrated and spatially correlated with signal detections from nearby pixels.

We approach the censoring of noise photons by computing the rank-ordered absolute differences (ROAD) statistic \cite{sci19} for each pixel. For each photon, we use the photon arrival times of the eight nearest neighbors \cite{sci16} and then perform a binary hypothesis test to decide whether the photon is due to background or noise with high probability. This binary hypothesis test is dependent on the reflectivity of the object at that pixel, which we recovered in the previous step (this is the reason why noise censoring cannot be done prior to reflectivity reconstruction, although it may be possible to consider an iterative procedure). We then delete the anomalous range values and replace them with averages of the neighbors before performing depth reconstruction.

The ROAD statistic is calculated as follows: for each pixel $(x,y)$, we take the eight neighboring pixels $(x_1,y_1) ... (x_8,y_8)$ and compute the time-of-arrival differences with the current pixel:
\begin{equation}
|t(x_1,y_1) - t(x,y)| , \, ... \, , |t(x_8,y_8) - t(x,y)|
\end{equation}
We then sort these values in ascending order, and define $\operatorname{ROAD}(x,y)$ to be the sum of the first four values in the sorted list.

We then use a binary hypothesis test to classify whether the photon arrival at $(x,y)$ is due to signal light or background light. We combine our knowledge of the reflectivity estimate $\alpha(x,y)$ which we obtained in the previous step with theory of merged Poisson Processes \cite{sci31} to obtain the probabilities:
\begin{equation}
\operatorname{Pr}[\mathrm{background}] = \frac{BT_r}{\alpha(x,y)S + BT_r}
\end{equation}
\begin{equation}
\operatorname{Pr}[\mathrm{signal}] = 1 - \frac{BT_r}{\alpha(x,y)S + BT_r}
\end{equation}

We then generate the threshold $C$ used for a binary hypothesis test as follows:
\begin{equation}
C = 4T_p \frac{ BT_r }{\alpha(x,y)S + BT_r}
\end{equation}
where if $\operatorname{ROAD}(x,y) \geq C$, we decide that the photon arrival is due to background light, delete the photon arrival and replace it with an average of its eight neighbors, and if $\operatorname{ROAD}(x,y) < C$, we keep the data point.

\subsubsection{Depth reconstruction}
The approach to reconstruction of the depth map of the object is similar to that of the reflectivity reconstruction approach. For each pixel $(x,y)$ we first compute the log-likelihood function relating the signal photon's arrival time $t(x,y)$ to the depth map of the scene $Z(x,y)$ in distance units. This is given by:
\begin{equation}
L(Z(x,y) | t(x,y)) = -\log\left[ s(t(x,y) - \frac{2Z(x,y)}{c}) \right]
\end{equation}
where $s(\cdot)$ is the pulse shape given earlier, $c$ is the speed of light, and the factor of 2 is incurred due to the round-trip distance from the emitter to the object and back to the detector. We insert the fitted $s(\cdot)$ we determined earlier to obtain:
\begin{equation}
L(Z(x,y) | t(x,y)) = -4\log\left[ t(x,y) - \frac{2Z(x,y)}{c} \right] - \frac{t(x,y) - T_s - 2Z(x,y)/c}{T_c}
\end{equation}
This negative log-likelihood function is strictly convex in $Z(x,y)$. Note that this is only convex for certain classes of pulse shapes $s(\cdot)$; our skewed Gamma function both fits our actual pulse shape and maintains convexity in this case.

Similar to the reflectivity construction, we measure sparsity using the $\ell_1$-norm of the wavelet transform of $Z(x,y)$ and reconstruct the depth by minimizing
\begin{equation}
(1 - \beta) \left[ \sum_x \sum_y L( Z(x,y) | t(x,y) ) \right] + \beta || \Phi\left(\left\{ Z(x,y) \right\}\right) ||_1
\end{equation}
subject to $Z(x,y) \geq 0$.

\subsubsection{Software implementation}
We employ the SPIRAL-TAP package for MATLAB \cite{sci18} to implement the optimization programs described in steps 1 and 3 above. Step 2 is implemented directly in MATLAB. For the starting point of the convex optimizer used for step 1, we use the pointwise ML estimate as a starting point:
\begin{equation}
\alpha_{ML}(x,y) = \operatorname{max}\left\{ \frac{1}{\gamma S (n(x,y) - 1)} - \frac{BT_r}{S}, 0 \right\}
\end{equation}

For the starting point of the depth estimator, we use the ML estimate
\begin{equation}
Z_{ML}(x,y) = c(t(x,y) - T_m)
\end{equation}
where $T_m = \operatorname{arg\,max} s(t)$ as long as the detection at pixel $(x,y)$ was determined in step 2 to be due to a signal photon. For noise pixels we input $Z(x,y) = 0$ as the starting point to the optimization routine. Full code and sample data are available with our published paper \ref{kirmani-first}.

\subsection{Results}

\subsubsection{Mannequin}
Our main imaging target was a life-size plastic mannequin wearing an black T-shirt with white text. We chose this not only as an example of a realistic target, but also as one which has both depth and intensity features on multiple scales.

\begin{figure}[htb]
\centerline{\includegraphics[width=16cm]{figure-first-mannequin.pdf}}
\caption{3D mesh views of a single first-photon acquisition after various stages of processing. (A)-(C): Raw reflectivity ML estimate $\alpha_{ML}(x,y)$ superimposed over depth ML estimate $Z_{ML}(x,y)$ from first photon data, (D)-(F): after reflectivity reconstruction of $\alpha(x,y)$, (G)-(I): after background noise censoring, (J)-(L): after depth reconstruction of $Z(x,y)$.}
\label{figure:first-mannequin}
\end{figure}

\begin{figure}[htb]
\centerline{\includegraphics[width=16cm]{figure-first-mannequin-zoom.pdf}}
\caption{Close-up of two regions of reflectivity data for mannequin target. (a) ML estimate using only 1 photon per pixel and (b) the same data after processing using our computational reconstruction technique.}
\label{figure:first-mannequin-zoom}
\end{figure}

Figure \ref{figure:first-mannequin} shows reflectivity superimposed on a 3D scatter plot of the depth data from raw data to each step of data processing described above; all pictures are generated from the same data set consisting of one photon detection per pixel. We notice that the reflectivity reconstruction is able to recover sufficient detail to resolve the text on the T-shirt, a close-up of which is shown in figure \ref{figure:first-mannequin-zoom}. Background noise censoring and depth reconstruction also performed spectacularly, being able to recover many facial feature details almost impossible to make out in the raw data.

\subsubsection{Depth chart}

To independently analyze the depth-resolving capabilities of our computational reconstruction technique, we constructed and imaged a depth chart consisting of a flat board with 5$\times$5 embossed rectangular plastic squares, as shown in figure \ref{figure:first-depthchart}. Each square had transverse dimensions of 5$\times$5 cm and the axial heights linearly increased in steps of $\sim$3.2 mm along both axes and the entire board was painted using a single color of paint for near-constant reflectivity. We first took a reference ("ground truth") measurement by accumulating and averaging hundreds of arrivals at each pixel, as shown in \ref{figure:first-depthchart}(a), which clearly shows all 25 squares. Although the board is flat, the depth images incur spherical distortion due to angular raster scanning.

Figure \ref{figure:first-depthchart}(b) shows the ML estimate of the depth taken only from the first photon arrival at each pixel. Since our axial feature size is on the order of $\sim$3.2 mm and our RMS pulse duration is $\sim$226 ps which multiplied by the speed of light corresponds to 67 mm, it is natural to expect that objects significantly smaller than the pulse width would be almost impossible to resolve without averaging; this is indeed the case with the raw ML estimate. However, as we see in figure \ref{figure:first-depthchart}(c), the same data processed through our computational reconstruction technique demonstrates the ability to resolve squares as small as 6.4 mm, which demonstrates sub-pulse depth resolution by over a factor of 10. This is explained by the fact that the ML estimates do not take into account spatial regularity, whereas our computational reconstruction approach promotes smooth surfaces with few sharp transitions, making use of data from several pixels at once to extract these features that are otherwise almost invisible.

\begin{figure}[htb]
\centerline{\includegraphics[height=20cm]{figure-first-depthchart.pdf}}
\caption{Depth chart test consisting of square features of linearly increasing height. (a) Image produced by traditional averaging with several hundred detections per pixel, (b) ML estimate using only the first photon arrival, and (c) Data in (b) processed using our computational reconstruction method.}
\label{figure:first-depthchart}
\end{figure}

\subsubsection{Reflectivity chart}

We also created a chart to independently analyze the reflectivity-resolving capabilities of our imager. As shown in figure \ref{figure:first-intensitychart}, the chart consists of printed greyscale squares arranged in rows of 2, 4, 8, and 16 shades on linear scales, corresponding to 1, 2, 3, and 4 bits of reflectivity resolution, respectively. Figure \ref{figure:first-intensitychart}(a) shows the ground truth measurement obtained by averaging the number of photon arrivals over a long dwell time, and is essentially a greyscale photograph in the traditional sense. Figure \ref{figure:first-intensitychart}(b) shows the ML estimate of the reflectivity from only the first photon arrival given Poisson statistics, which results in an extremely noisy image since the error on the ML estimate is high. Figure \ref{figure:first-intensitychart}(c) shows the same first-photon data processed using our reconstruction technique. We see that we can clearly resolve between 8 and 16 shades of grey, corresponding to $\sim$3 to 4 bits of reflectivity resolution. Again, although it may seem counterintuitive to obtain such high reflectivity resolution using only 1 photon arrival at each pixel, we are exploiting sparsity in the scene and promoting spatially-smooth objects, making use of several nearby pixels of data at once, in contrast to traditional averaging methods which treat each pixel as an independent process.

\begin{figure}[htb]
\centerline{\includegraphics[height=20cm]{figure-first-intensitychart.pdf}}
\caption{Reflectivity chart test. (a) Image produced by traditional averaging with several hundred detections per pixel, (b) ML estimate using only the first photon arrival, and (c) Data in (b) processed using our computational reconstruction method.}
\label{figure:first-intensitychart}
\end{figure}

\section{Imaging using a SPAD array}

We now turn to the question of single-photon imaging without raster scanning. Although there are no high-resolution commercially-available single photon detector arrays as of the time of this writing, necessitating raster scanning, it would be highly desirable to be able to image an entire scene simultaneously without raster scanning each pixel one at a time. This would enable an acquisition speedup by as much as a factor of the number of pixels, enabling possibilities for real-time video and other commercial, scientific, or military applications.

Although not yet commercially available, single-photon avalanche photodiode (SPAD) arrays are currently a popular topic of research. F. Villa et al. at the Politecnico di Milano researched and developed an 32$\times$32-pixel SPAD array \cite{villa-thesis} that features 6-bit photon-counting and 10-bit time-of-flight ranging modes. We formed a collaboration with principal investigator F. Zappa and invited graduate student R. Lussano to bring two prototypes of the SPAD array to MIT to test with our computational reconstruction method.

\subsection{Hardware specifications}
The SPAD array developed by the Zappa group is a 32$\times$32-pixel array of fully independent CMOS SPAD detectors. Each pixel is 150$\times$150 $\mu$m in size and has a circular active region with diameter 30 $\mu$m, giving a fill factor of 3.14\%. The photon detection efficiency at 640 nm is $\sim$20\% which ives a combined detection efficiency of 0.63\%. Although the efficiency of detection is extremely low, we understand that this is an early-stage prototype device and expect that future research in SPAD arrays will be able to achieve higher fill factors and detection efficiencies.

The dark count rate of the SPAD array is specified at $\sim$100 cps, but we found that in reality the dark count rate varies highly across pixels. In particular a small number of "hot" pixels have extremely high dark count rates (in excess of 500 cps) which we attribute to defects in the custom fabrication of the array.

The SPAD array was housed inside a metal enclosure with a 1-inch diameter opening.

\subsubsection{Modes of operation}

The array has two modes of operation: counting mode and timing mode. In counting mode, the 6-bit registers located at each detector site are incremented each time a photon arrival is detected, allowing for upto 63 photon arrivals (binary 111111) before rolling over to 0. Hence in counting mode it is important to keep the power level low enough to avoid this rollover. Alternatively, in the case of smoothly-varying objects, it may be possible to unwrap the rollover, similar to 1-dimensional phase-unwrapping algorithms.

In timing mode, the 6 bits of the array are used for recording the time of arrival of the first photon arrival at each pixel with a global electronic shutter. An additional 4-bit register using a hardware timing interpolation scheme provides for a total of 10 bits of timing resolution (1024 bins). The minimum time bin size is $\sim$312 ps and is determined by the clock rate. Due to anomalies in measurementes close to this limit, we operate at closer to $\sim$400 ps. It should be noted that unlike the single photon detector and HydraHarp setup, the SPAD array neds additional time to save its measurements and reset the registers, which takes a minimum of 10 $\mu$s per frame. This limits the maximum frame rate of acquisition to around 100 kfps in burst mode, in which a short number of frames are acquired and data stored on the SPAD array itself until subsequent offloading to a PC. In continuous acquisition mode, the USB link limits us to an acquisition rate of around 10 kfps.

The timing acquistion window of the SPAD array is 1024 bins of size $\sim$400 ps, or $\sim$400 ns. Since the length of one frame of data acquisition can be no shorter than 10 $\mu$s, this severely impacts our measurement duty cycle. In order to alleviate this, the SPAD array also features a "refolding mode" in which upto 8 back-to-back laser pulses are shot and synchronized measurements are performed without resetting the registers: if a photon is detected at a particular pixel, that pixel will no longer receive another detection until the end of the 8 pulses; if no photon is detected, that pixel will continue to be active and available for recording a photon arrival until the end of the 8 pulses. For depth resolution experiments this is helpful since we have $\sim$8 times the probability of detecting a photon at each pixel, and only need to know the time of arrival of a pulse relative to the previous laser shot pulse, which is what the hardware automatically records. However, in refolding mode the hardware does not provide information about which of the 8 pulses a particular photon detection corresponds to; thus, for reflectivity measurements it is important to ensure that the power level is not so high such a photon is almost always detected for each set of 8 pulses, as this would cause us to lose all information about reflectivity. (An alternate approach would be to simply switch the SPAD array to counting mode to perform reflectivity measurements; however this would necessitate two acquisitions to obtain both depth and intensity information about an object, and severely impact our ability to perform high frame-rate measurements if we want to image moving objects.)

\subsection{Experimental setup}
A diagram of the experimental setup is shown in figure \ref{figure:first-spad-setup}. The SPAD array has 32$\times$32 pixels each with a pitch of 150 $\mu$m, giving a total array size of 4.8$\times$4.8 mm. In principle we would like both a larger imaging area as well as a higher number of pixels in order to experiment with computational reconstruction methods. Given the inavailability of higher resolution SPAD arrays at this time, we opted to mount the SPAD array on a feedback-controlled motorized translation stage and translate over exactly 5$\times$5 tiles of the 4.8$\times$4.8 mm array, as shown in figure \ref{figure:first-spad-scanning}(a), giving us a total imaging area of 24$\times$24 mm. Since the active area of each pixel is only 30 $\mu$m in diameter, which is less than 1/4 of a pixel pitch distance, we also increase resolution by subsampling at each pixel by translating the SPAD array along both axes to all combinations of ${0, 1/4, 1/2, 3/4}$ of a pixel pitch length along the x- and y-axes before moving onto the next tile, as shown in figure \ref{figure:first-spad-scanning}(b)-(d). In other words, we translate the SPAD array to all combinations of positions $(x, y) = (L_i + \Delta_j, L_k + \Delta_l)$ for all combinations of $L_i, L_k \in \{ 0, 4.8, 9.6, 14.4, 19.2, 24\}$ (tile positions) and $\Delta_j, \Delta_k \in \{ 0, .0375, .0750, .1125\}$ (subpixel sampling) where all dimensions are in mm. This gives us a total imaging resolution of $32 \times 5 \times 4$ = $768$ pixels along each axis and requires a total of $5 \times 5 \times 4 \times 4$ = 400 total acquisitions to produce the $768\times 768$ image. Although this requires some acquisition time, it is still much faster than raster scanning over single pixels which would require $768^2 = 518400$ acquisitions for the same resolution image; in addition we expect higher-resolution SPAD arrays to be available in the future.

\begin{figure}[h!]
\centerline{\includegraphics[width=15cm]{figure-first-spad-setup.pdf}}
\caption{Experimental setup for single-photon imaging with a SPAD array.}
\label{figure:first-spad-setup}
\end{figure}

The MATLAB user interface for the SPAD array was modified to automate the acquisition procedure and synchronously drive the translation stages to each of the the 400 positions, outputting a separate data file for each position containing photon arrival data.

The SPAD firmware outputs the values of all SPAD registers at each acquisition frame. However, since in each frame there are many pixels that do not contain arrival data, this is an inefficient method to store and process data as a single acquisition would occupy upto several gigabytes. In addition, the output of the SPAD array pixels is not in order due to the design of the electronics. In order to speed up data processing, we developed a custom GNU C program (see Appendix \ref{appendix:spadcounts}) to read the SPAD array binary files and output a cleaner, MATLAB-friendly file containing only a sequence of photon detections tagged by pixel number and time frame number. The C program also both ASCII and binary output modes, both of which are conveniently and efficiently readable in MATLAB.

Mounted in front of the translatable SPAD array was a standard Canon FL-series photographic lens with a focal length of 55 mm and a maximum photographic aperture of f/1.2. We set the aperture of the lens to f/2.8 which provided the necessary depth of field to capture sufficient detail from various depth ranges of our object, increase sharpness, and reduce vignetting. The lens, designed for 35 mm film cameras, had an image circle of slightly larger than 35 mm, allowing us to mount the lens at a fixed position and conveniently fit our $24\times24$ mm square imaging area entirely within the image circle of the lens. Basic tests in counting mode showed that at f/2.8, the lens was able to easily and clearly resolve objects as small as one single pixel in the $768\times768$ pixel image with almost zero cross-talk, provided that the lens was manually focused correctly.

\begin{figure}[h!]
\centerline{\includegraphics[width=15cm]{figure-first-spad-scanning.pdf}}
\caption{Scanning scheme to obtain high-resolution images using the 32$\times$32-pixel SPAD array. (a) We translate the array by increments of a full array size (4.8 mm) along both axes to image multiple "tiles" of the entire array. (b) Zoom-in view of individual SPAD pixels showing the active area with no subpixel scanning, (c) 2$\times$2 subpixel scanning by translating along each axis by 75 $\mu$m, multiplying the resolution by 2 and (d) 4$\times$4 subpixel scanning by translating along each axis by 37.5 $\mu$m, 75 $\mu$m, and 112.5 $\mu$m, multiplying the resolution by 4.}
\label{figure:first-spad-scanning}
\end{figure}

\subsection{Computational reconstruction approach}

Unlike first photon imaging where we guarantee an arrival at every pixel, when imaging with a SPAD array it is necessary to fix a dwell time across the entire image, implying that for very dark regions it is possible that no photon arrivals will be measured. On the other hand, it is also possible that for bright regions, multiple arrivals will be measured in one dwell time period, and it is important that we make reasonable use of this additional information rather than discarding it. Kirmani et. al. \cite{kirmani-photon} adapted the three-step algorithm above to the fixed-dwell time case which we briefly summarize here.

\subsubsection{Reflectivity reconstruction}
In first-photon imaging we reconstruct reflectivity using the number of laser pulses $n(x,y)$ before the first detected photon and the fact that the maximum-likelihood estimate of the intensity based on this information is proportional to $1/n$ based on Poisson statistics. In the case of a fixed dwell time of $N$ laser pulses per pixel, we simply measure the total number of photon detections $k(x,y)$ for each pixel and set adjust our log-likelihood function to
\begin{equation}
L\left( \alpha(x,y) | k(x,y) \right) = \gamma \left[ \alpha(x,y) S + B T_r \right] \left[ N - k(x,y) \right] - k(x,y) \log \left[ 1 - exp \left[ - \gamma \alpha(x,y) S + B T_r \right] \right]
\end{equation}
This function is also strictly convex in $\alpha(x,y)$ and we proceed normally using the same computational reconstruction program as before.

\subsubsection{Background noise rejection}
Since for each pixel we obtain not exactly one arrival, but anywhere from $0$ to several arrivals (in the low-flux regime we neglect the possibility that multiple arrivals occur in a single laser pulse), our background noise censoring needs to be adjusted accordingly. Kirmani et. al. \cite{kirmani-photon} suggest that given a set of arrivals $\left\{ t_\ell(x,y) \right\}$ at pixel $(x,y)$, we compute the rank-ordered mean (ROM) $t_{ROM}(x,y)$ which is the median value of the detection times of all the detections acquired in the neighboring 8 pixels, setting $t_{ROM}(x,y) = \infty$ in the case of missing data. Following this, the set of indices of uncensored detections $U(x,y)$ at pixel $(x,y)$ is defined by
\begin{equation}
U(x,y) = \left\{ \ell : |t_\ell(x,y) - t_{ROM}(x,y)| < 2 T_p \left( \frac{BT_r}{\gamma \alpha(x,y)S + BT_r}  \right), 0 \leq \ell < k(x,y) \right\}
\end{equation}
using the estimate $\alpha(x,y)$ from the result of the optimization in the first step.

\subsubsection{Depth reconstruction}
We now proceed to depth reconstruction. If $U(x,y)$ is is non-empty, we set the log-likelihood function for depth reconstruction by summing over the log likelihoods of all uncensored detections:
\begin{equation}
L\left( Z(x,y) | \left\{t_\ell(x,y) | \ell \in U(x,y) \right\} \right) = - \sum_{\ell \in U(x,y)} \log\left[ s(t_\ell(x,y) - \frac{2Z(x,y)}{c}) \right]
\end{equation}
If $U(x,y)$ is empty, i.e. if after background censoring, no data is available for pixel $(x,y)$, we set the value of the log-likelihood function to $0$ so that it does not contribute to the scene's overall log-likelihood cost function \cite{kirmani-photon}. We then proceed with the optimization program as before. Optimization procedures for fixed dwell time imaging were similarly executed using SPIRAL-TAP for MATLAB.

\subsection{Preliminary results}

Figures \ref{figure:first-spad-pvnrt-i} shows the results of our reconstruction approach for data taken with the same mannequin used for first-photon imaging but wearing a T-shirt with larger text and a colored background. Figure \ref{figure:first-spad-bball-i} is of two basketballs on a table and \ref{figure:first-spad-boxbooks-i} is of several books and a cardboard box on a table. Although we took data with full 4$\times$4 subpixel scans (768$\times$768 total image pixels), for this thesis we only analyzed data from 2$\times$2 subscans (360$\times$360) since in the full subscan data, hot pixels also appear in groups of 4$\times$4 which interferes with our sparsity-promoting regularization since they effectively get recognized and emphasized as 4$\times$4-pixel features. A more advanced algorithm may be able to specifically recognize and mitigate this situation by performing aggressive range gating based on nearby pixel data when a 4$\times$4-pixel group of hot pixels is detected.

\subsection{Comparison to first-photon imaging}

We notice that for the reflectivity measurements, we see that similar to first-photon imaging, we obtain cleaner images. However unlike first-photon imaging, the dynamic range of reflectivity measurements using SPAD array is limited, since in fixed dwell-time operation, if we tune the dwell time such that the bright regions receive on the order of $\sim$1-5 photons, we almost no data for darker regions (including, for example, the black background cloth behind the mannequin) since the lack of any signal photon arrivals prevents us from gaining any information about reflectivity variations. This is in stark contrast to first-photon imaging in which an arrival at each pixel is guaranteed.

If we extend the dwell time long enough such that we receive at least one arrival for most pixels, we obtain clean images, but also note that in this case, there are sufficient arrivals in the bright regions to simply use traditional averaging with excellent results, reducing the need for a sparsity-based approach to reconstruct images.

We do note that the level of improvement obtained in fixed dwell time imaging is much less than that of first-photon imaging. We also confirmed this by post-proccessing data from the SPAD arrays to simulate first-photon imaging, in which we gather data for an extended period of time but only keep the first photon arrival at every pixel; the results are shown in figure \ref{figure-spad-first-i} for two scenes, demonstrating again the level of improvement seen using the raster scanning equipment earlier, including the ability to recover text that is almost illegible from the raw ML estimates.

\begin{figure}[h!]
\centerline{\includegraphics[width=15cm]{figure-first-spad-pvnrt-i.pdf}}
\caption{SPAD array imaging results for 360$\times$360-pixel reflectivity images of a mannequin comparing traditional averaging with spatial regularization. Dwell times are in time units of 16384 SPAD acquisition frames (6.542 ms).}
\label{figure:first-first-spad-pvnrt-i}
\end{figure}

\begin{figure}[h!]
\centerline{\includegraphics[width=15cm]{figure-first-spad-boxbooks-i.pdf}}
\caption{SPAD array imaging results for 360$\times$360-pixel reflectivity images of a table, cardboard box, and books comparing traditional averaging with spatial regularization. Dwell times are in time units of 16384 SPAD acquisition frames (6.542 ms).}
\label{figure:first-first-spad-boxbooks-i}
\end{figure}

\begin{figure}[h!]
\centerline{\includegraphics[width=15cm]{figure-first-spad-bball-i.pdf}}
\caption{SPAD array imaging results for 360$\times$360-pixel reflectivity images of two basketballs comparing traditional averaging with spatial regularization. Dwell times are in time units of 16384 SPAD acquisition frames (6.542 ms).}
\label{figure:first-first-spad-boxbooks-i}
\end{figure}

\begin{figure}[h!]
\centerline{\includegraphics[width=11cm]{figure-first-spad-first-i.pdf}}
\caption{First-photon reflectivity imaging of three scenes simulated using the SPAD array, in which only the first arrival at each pixel is used regardless of subsequent arrivals within a fixed dwell time. (a) Traditional ML estimate of the reflectivity and (b) the same data after processing using our first-photon imaging technique described in the previous section. Depth results are not generated since $s(\cdot)$ is nearly a delta function for the laser pulse width and time bin size used.}
\label{figure:first-first-spad-pvnrtfirst-i}
\end{figure}

We found that with our imaging parameters and object size, computational reconstruction provided little benefit to the depth images other than successfully removing noise pixels; few additional features were recovered that were not visible in the raw data. We attribute this to a mismatch between the SPAD specifications, laser pulse width, and the imaging target. In contrast to our first-photon imaging experiment in which the time bin size of the HydraHarp is much smaller than the width of the laser pulse, in this case the time bin size of the SPAD array was actually much larger. This means that $s(\cdot)$ is effectively only 1-2 time bins in size and an object with small features in the axial direction becomes nearly obliterated by the quantization error of the bin size. We expect that the particular SPAD array we used would lend itself to more fruitful depth reconstruction experiments if we used a laser pulse much longer than the time bin size, e.g. at least 1-2 ns, and a large object with depth features at least 1/10 of the pulse width. Under these conditions we expect to see a stronger difference between the depth-resolving capabilities of traditional imaging and our sparsity-based reconstruction technique.

\section{Compressed single-photon imaging using a digital micromirror device}
Digital micromirror devices (DMDs) are arrays of tiny programmable mirrors and widely used in commercial projectors since they offer extremely high contrast and high-speed (upto tens of kHz) switching rates. They are also of particular use in sensing applications since they can be used not only to shape optical beam intensity profiles but also can be placed in front of a single-pixel detector and used to implement single-pixel cameras and time-of-flight cameras.

In this section we pose the question of performing compressed imaging experiments similar to that of the single-pixel camera but using a single-photon APD insted of a classical detector. We also collect data for development of new compressed sensing algorithms that take into account the physics of single-photon detection and operate at a light level where measurements are corrupted by Poisson noise.

\subsection{Experimental setup}

\subsection{Preliminary results}

\subsection{Future research}

\section{Conclusions}
